# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/00_core.ipynb.

# %% auto 0
__all__ = ['ResizeMax', 'PadSquare', 'CustomTrivialAugmentWide', 'CustomRandomIoUCrop']

# %% ../nbs/00_core.ipynb 4
from typing import Any, Dict, Optional, List, Tuple
import random

from cjm_pil_utils.core import stack_imgs
from cjm_pytorch_utils.core import pil_to_tensor, tensor_to_pil

import numpy as np

from PIL import Image

# Import PyTorch dependencies
import torch
import torchvision
torchvision.disable_beta_transforms_warning()
from torchvision.tv_tensors import BoundingBoxes
from torchvision.utils import draw_bounding_boxes
import torchvision.transforms.v2  as transforms
from torchvision.transforms.v2 import functional as TF

from torchvision.transforms.v2._utils import query_size, get_bounding_boxes
from torchvision.ops.boxes import box_iou
from torchvision.ops import box_convert
from torchvision.tv_tensors import BoundingBoxFormat

from torchvision.utils import draw_bounding_boxes

# %% ../nbs/00_core.ipynb 7
class ResizeMax(transforms.Transform):
    """
    A PyTorch Transform class that resizes an image such that the maximum dimension 
    is equal to a specified size while maintaining the aspect ratio.
    """
    
    def __init__(self, 
                 max_sz:int=256 # The maximum size for any dimension (height or width) of the image.
                ):
        """
        Initialize ResizeMax object with a specified max_sz. 
        """
        # Call to the parent class (Transform) constructor
        super().__init__()

        # Set the maximum size for any dimension of the image
        self.max_sz = max_sz
        
    def _transform(self, 
                   inpt: Any, # The input image tensor to be resized.
                   params: Dict[str, Any] # A dictionary of parameters. Not used in this method but is present for compatibility with the parent's method signature.
                  ) -> torch.Tensor: # The resized image tensor.
        """
        Apply the ResizeMax transformation on an input image tensor.
        """

        # Copy the input tensor to a new variable
        x = inpt

        # Get the width and height of the image tensor
        spatial_size = TF.get_size(x)

        # Calculate the size for the smaller dimension, such that the aspect ratio 
        # of the image is maintained when the larger dimension is resized to max_sz
        size = int(min(spatial_size) / (max(spatial_size) / self.max_sz))

        # Resize the image tensor with antialiasing for smoother output
        x = TF.resize(x, size=size, antialias=True)

        # Return the transformed (resized) image tensor
        return x

# %% ../nbs/00_core.ipynb 11
class PadSquare(transforms.Transform):
    """
    PadSquare is a PyTorch Transform class used to pad images to make them square. 
    Depending on the configuration, padding can be applied equally on both sides, 
    or can be randomly split between the two sides.
    """

    def __init__(self, 
                 padding_mode:str='constant', # The method to use for padding. Default is 'constant'.
                 fill:tuple=(123, 117, 104), # The RGB values to use for padding if padding_mode is 'constant'.
                 shift:bool=True # If True, padding is randomly split between the two sides. If False, padding is equally applied.
                ):
        """
        The constructor for PadSquare class.
        """
        super().__init__()
        self.padding_mode = padding_mode
        self.fill = fill
        self.shift = shift
        self.pad_split = None

    def forward(self, 
                *inputs: Any # The inputs to the forward method.
               ) -> Any: # The result of the superclass forward method.
        """
        The forward method that sets up the padding split factor if 'shift' is True, 
        and then calls the superclass forward method.
        """
        self.pad_split = random.random() if self.shift else None
        return super().forward(*inputs)

    def _transform(self, 
                   inpt: Any, # The input to be transformed.
                   params: Dict[str, Any] # A dictionary of parameters for the transformation.
                  ) -> Any: # The transformed input.
        """
        The _transform method that applies padding to the input to make it square.
        """
        x = inpt
        
        # Get the width and height of the image tensor
        h, w = TF.get_size(x)
        
        # If shift is true, padding is randomly split between two sides
        if self.shift:
            offset = (max(w, h) - min(w, h))
            pad_1 = int(offset*self.pad_split)
            pad_2 = offset - pad_1
            
            # The padding is applied to the shorter dimension of the image
            self.padding = [0, pad_1, 0, pad_2] if h < w else [pad_1, 0, pad_2, 0]
            padding = self.padding
        else:
            # If shift is false, padding is equally split between two sides
            offset = (max(w, h) - min(w, h)) // 2
            padding = [0, offset] if h < w else [offset, 0]
        
        # Apply the padding to the image
        x = TF.pad(x, padding=padding, padding_mode=self.padding_mode, fill=self.fill)
        
        return x

# %% ../nbs/00_core.ipynb 14
class CustomTrivialAugmentWide(torchvision.transforms.TrivialAugmentWide):
    """
    This class extends the TrivialAugmentWide class provided by PyTorch's transforms module.
    TrivialAugmentWide is an augmentation policy randomly applies a single augmentation to each image.
    """
    def __init__(
        self,
        num_magnitude_bins: int = 31,
        interpolation: transforms.InterpolationMode = transforms.InterpolationMode.NEAREST,
        fill: Optional[List[float]] = None,
        op_meta: Optional[Dict[str, Tuple[torch.Tensor, bool]]] = None
    ) -> None:
        super().__init__(num_magnitude_bins, interpolation, fill)
        self.op_meta = op_meta if op_meta else super()._augmentation_space(num_bins)

    def _augmentation_space(self, 
                            num_bins: int
                           ) -> Dict[str, Tuple[torch.Tensor, bool]]:
        return self.op_meta

# %% ../nbs/00_core.ipynb 18
class CustomRandomIoUCrop(transforms.RandomIoUCrop):
    
    """
    A customized Random IoU crop transformation that inherits from torchvision's RandomIoUCrop transform.
    """
    
    def __init__(
        self,
        min_scale: float = 0.3, # Minimum factors to scale the input size.
        max_scale: float = 1.0, # Maximum factors to scale the input size.
        min_aspect_ratio: float = 0.5, # Minimum aspect ratio for the cropped image or video.
        max_aspect_ratio: float = 2.0, # Maximum aspect ratio for the cropped image or video.
        sampler_options: Optional[List[float]] = [0.0, 0.1, 0.3, 0.5, 0.7, 0.9, 1.0], # List of minimal IoU (Jaccard) overlap between all the boxes a cropped image or video.
        trials: int = 40, # Number of trials to find a crop for a given value of minimal IoU (Jaccard) overlap.
        jitter_factor: float = 0.0 # Value to jitter the center coordinates for the crop area.
    ):
        super().__init__(min_scale, max_scale, min_aspect_ratio, max_aspect_ratio, sampler_options, trials)
        self.jitter_factor = jitter_factor
    
    def _get_params(self, flat_inputs: List[Any]) -> Dict[str, Any]:
        orig_h, orig_w = query_size(flat_inputs)
        bboxes = get_bounding_boxes(flat_inputs)
        
        bbox_dims = bboxes[...,2:]

        # Get all bbox centers
        xyxy_bboxes_orig = box_convert(
            bboxes.as_subclass(torch.Tensor), bboxes.format.value.lower(), torchvision.tv_tensors.BoundingBoxFormat.XYXY.value.lower()
        )

        cx = 0.5 * (xyxy_bboxes_orig[..., 0] + xyxy_bboxes_orig[..., 2])
        cy = 0.5 * (xyxy_bboxes_orig[..., 1] + xyxy_bboxes_orig[..., 3])
        
        for _ in range(self.trials):
            # Sample an option
            idx = int(torch.randint(low=0, high=len(self.options), size=(1,)))
            min_jaccard_overlap = self.options[idx]
            if min_jaccard_overlap >= 1.0:
                return dict()

            # Check aspect ratio limitations and generate new width and height
            while True:
                r = self.min_scale + (self.max_scale - self.min_scale) * torch.rand(2)
                new_w = int(orig_w * r[0])
                new_h = int(orig_h * r[1])
                aspect_ratio = new_w / new_h
                if self.min_aspect_ratio <= aspect_ratio <= self.max_aspect_ratio:
                    break

            # Bounding Box Guided Sampling
            center_idx = torch.randint(low=0, high=cx.shape[0], size=(1,))
            crop_center_x = cx[center_idx]
            crop_center_y = cy[center_idx]
            
            # Add jitter to the crop centers
            jitter_val_x = self.jitter_factor * bbox_dims[center_idx.item()][0].item() * torch.randn(1)
            jitter_val_y = self.jitter_factor * bbox_dims[center_idx.item()][1].item() * torch.randn(1)

            crop_center_x += jitter_val_x
            crop_center_y += jitter_val_y

            left = int(crop_center_x - new_w / 2)
            top = int(crop_center_y - new_h / 2)
            right = left + new_w
            bottom = top + new_h

            # Adjust if out of bounds
            if left < 0: left, right = 0, new_w
            if right > orig_w: right, left = orig_w, orig_w - new_w
            if top < 0: top, bottom = 0, new_h
            if bottom > orig_h: bottom, top = orig_h, orig_h - new_h

            is_within_crop_area = (left < cx) & (cx < right) & (top < cy) & (cy < bottom)
            if not is_within_crop_area.any():
                continue

            # Check at least 1 box with jaccard limitations
            xyxy_bboxes = xyxy_bboxes_orig[is_within_crop_area]
            ious = box_iou(
                xyxy_bboxes,
                torch.tensor([[left, top, right, bottom]], dtype=xyxy_bboxes.dtype, device=xyxy_bboxes.device),
            )
                        
            if ious.max() < min_jaccard_overlap:
                continue
            
            return dict(top=top, 
                        left=left, 
                        height=new_h, 
                        width=new_w, 
                        is_within_crop_area=is_within_crop_area)
        return dict()
