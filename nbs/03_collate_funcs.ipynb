{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collate Functions\n",
    "\n",
    "> Some custom collate functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp collate_funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import math\n",
    "from typing import Any, Dict, Optional, List, Tuple, Union\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "# Import PyTorch dependencies\n",
    "import torch\n",
    "import torchvision\n",
    "torchvision.disable_beta_transforms_warning()\n",
    "from torchvision.tv_tensors import BoundingBoxes, Mask\n",
    "import torchvision.transforms.v2  as transforms\n",
    "from torchvision.transforms.v2 import functional as TF\n",
    "\n",
    "from cjm_torchvision_tfms.transforms import ResizeMax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def round_up_to_multiple(val, divisor=32):\n",
    "    \"\"\"Round val up to the nearest multiple of `divisor`.\"\"\"\n",
    "    return math.ceil(val / divisor) * divisor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def resize_pad_collate(batch, max_sz=256):\n",
    "    \"\"\"\n",
    "    A custom collate function for a PyTorch DataLoader that performs:\n",
    "    \n",
    "    1) **Resize each (image, target) pair** so that the image's maximum dimension \n",
    "       does not exceed `max_sz` (using the custom `ResizeMax` transform).\n",
    "    2) **Determine the largest image height and width in the batch**, \n",
    "       round them up to a multiple of 32, and then **randomly pad each image** \n",
    "       so they all share the same dimensions. BoundingBoxes and Masks in the \n",
    "       targets are updated accordingly.\n",
    "    3) **Optionally perform a final Resize** to ensure all images have the \n",
    "       same shape (often a no-op if the padded size already matches).\n",
    "    \n",
    "    Args:\n",
    "        batch (List[Tuple[Image, Dict]]): \n",
    "            A list of (image, target) pairs, where:\n",
    "              - `image` can be a PIL Image, PyTorch tensor, or TorchVision `tv_tensors.Image`.\n",
    "              - `target` is typically a dictionary containing bounding boxes (`\"boxes\"`), \n",
    "                masks (`\"masks\"`), and possibly other metadata.\n",
    "        max_sz (int, optional): \n",
    "            The maximum size (height or width) for the resize step. Default: `256`.\n",
    "    \n",
    "    Returns:\n",
    "        List[Tuple[Image, Dict]]: \n",
    "            A list of (image, target) pairs where each image is resized and padded \n",
    "            to the same dimensions, and any bounding boxes or masks have been \n",
    "            shifted/padded accordingly.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1) Resize each (image, target) pair so the image's largest side <= max_sz\n",
    "    resized_pairs = []\n",
    "    resize_max_transform = ResizeMax(max_sz=max_sz)\n",
    "    for (img, tgt) in batch:\n",
    "        new_img, new_tgt = resize_max_transform(img, tgt)\n",
    "        resized_pairs.append((new_img, new_tgt))\n",
    "\n",
    "    # Determine the largest height and width across all resized images\n",
    "    raw_max_height, raw_max_width = 0, 0\n",
    "    for (img, _) in resized_pairs:\n",
    "        h, w = TF.get_size(img)\n",
    "        raw_max_height = max(raw_max_height, h)\n",
    "        raw_max_width = max(raw_max_width, w)\n",
    "\n",
    "    # Round up these dimensions to the nearest multiple of 32\n",
    "    final_max_height = round_up_to_multiple(raw_max_height, 32)\n",
    "    final_max_width = round_up_to_multiple(raw_max_width, 32)\n",
    "\n",
    "    # Step 2) Randomly pad each image (and its target data) to match\n",
    "    #         (final_max_height, final_max_width)\n",
    "    padded_pairs = []\n",
    "    for (img, tgt) in resized_pairs:\n",
    "        h, w = TF.get_size(img)\n",
    "        \n",
    "        # How much total padding is needed for height/width\n",
    "        total_pad_h = final_max_height - h\n",
    "        total_pad_w = final_max_width - w\n",
    "\n",
    "        # Randomly distribute that padding (top vs. bottom, left vs. right)\n",
    "        top_pad = random.randint(0, total_pad_h)\n",
    "        bottom_pad = total_pad_h - top_pad\n",
    "        left_pad = random.randint(0, total_pad_w)\n",
    "        right_pad = total_pad_w - left_pad\n",
    "\n",
    "        # Pad the image; TF.pad() expects [left, top, right, bottom]\n",
    "        pad_img = TF.pad(img, [left_pad, top_pad, right_pad, bottom_pad])\n",
    "\n",
    "        # Adjust target dictionary if present\n",
    "        new_tgt = {}\n",
    "        if isinstance(tgt, dict):\n",
    "            # Shift bounding boxes\n",
    "            if \"boxes\" in tgt and isinstance(tgt[\"boxes\"], BoundingBoxes):\n",
    "                boxes_out = TF.pad(tgt[\"boxes\"], [left_pad, top_pad, right_pad, bottom_pad])\n",
    "                # Update the bounding box canvas size\n",
    "                boxes_out.canvas_size = (final_max_height, final_max_width)\n",
    "                new_tgt[\"boxes\"] = boxes_out\n",
    "\n",
    "            # Pad masks\n",
    "            if \"masks\" in tgt and isinstance(tgt[\"masks\"], Mask):\n",
    "                masks_out = TF.pad(tgt[\"masks\"], [left_pad, top_pad, right_pad, bottom_pad])\n",
    "                new_tgt[\"masks\"] = masks_out\n",
    "\n",
    "            # Copy over any other keys unchanged\n",
    "            for k, v in tgt.items():\n",
    "                if k not in (\"boxes\", \"masks\"):\n",
    "                    new_tgt[k] = v\n",
    "        else:\n",
    "            # If targets are not a dictionary, handle or copy directly\n",
    "            new_tgt = tgt\n",
    "\n",
    "        padded_pairs.append((pad_img, new_tgt))\n",
    "\n",
    "    # Step 3) Apply a final resize to ensure each image has \n",
    "    #         (final_max_height, final_max_width) dimensions.\n",
    "    #         Often a no-op if already padded to the exact size.\n",
    "    final_resize = transforms.Resize((final_max_height, final_max_width), antialias=True)\n",
    "    final_pairs = []\n",
    "    for (img, tgt) in padded_pairs:\n",
    "        out_img, out_tgt = final_resize(img, tgt)\n",
    "        final_pairs.append((out_img, out_tgt))\n",
    "\n",
    "    return final_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function: `resize_pad_collate(batch, max_sz=256)`\n",
    "\n",
    "A custom collate function designed for use in a PyTorch `DataLoader`. It processes a batch of `(image, target)` pairs by:\n",
    "\n",
    "1. **Resizing each image (and corresponding target data) so that the maximum dimension of the image does not exceed `max_sz`.**  \n",
    "   - Uses the custom `ResizeMax` transform (from `cjm_torchvision_tfms.transforms`) which maintains aspect ratio.\n",
    "\n",
    "2. **Determining the largest height and width in the batch, rounding them up to the nearest multiple of 32, and randomly padding each image to match those dimensions.**  \n",
    "   - Random padding is applied on all sides (top, bottom, left, right) of each image, ensuring that every image ends up with the same final size.\n",
    "   - Bounding boxes (`BoundingBoxes`) and masks (`Mask`) within each target dictionary are adjusted accordingly (shifted and padded).\n",
    "\n",
    "3. **Performing a final resize to guarantee that all images have the exact same dimensions.**  \n",
    "   - Typically a no-op if the padding already produces the correct size but ensures consistency in shape.\n",
    "\n",
    "### Arguments\n",
    "\n",
    "- **batch** (`List[Tuple[Image, dict]]`):  \n",
    "  A list of `(image, target)` pairs.  \n",
    "  - Each `image` can be a PIL image, PyTorch tensor image, or a TorchVision `tv_tensors.Image`.\n",
    "  - The `target` is typically a dictionary containing annotation data such as bounding boxes, masks, or other metadata. This function specifically looks for:\n",
    "    - `\"boxes\"` of type `tv_tensors.BoundingBoxes`\n",
    "    - `\"masks\"` of type `tv_tensors.Mask`\n",
    "    - Any other keys in the dictionary are passed through unchanged.\n",
    "\n",
    "- **max_sz** (`int`, optional, default=256):  \n",
    "  The maximum dimension (width or height) to which each image will be resized.  \n",
    "  - The aspect ratio of each image is preserved while resizing.\n",
    "\n",
    "### Returns\n",
    "\n",
    "- **final_pairs** (`List[Tuple[Image, dict]]`):  \n",
    "  A list of `(image, target)` pairs where:\n",
    "  1. Each image is at most `max_sz` in its largest dimension (before final padding).\n",
    "  2. Each image is then padded (randomly on all sides) so that all images share the same height and width (rounded up to multiples of 32).\n",
    "  3. The bounding boxes and masks in the target (if any) are updated to reflect the padding.\n",
    "  4. A final resize ensures that each image has the same dimensions (`(final_max_height, final_max_width)`).\n",
    "\n",
    "### How It Works\n",
    "\n",
    "1. **Resize to `max_sz`:**  \n",
    "   Uses `ResizeMax`, which shrinks the image so its larger side is at most `max_sz`, preserving the aspect ratio.\n",
    "\n",
    "2. **Identify maximum batch dimensions & pad:**\n",
    "   - Loops through all resized images to find the largest height and width.  \n",
    "   - Rounds them up to the nearest multiple of 32.  \n",
    "   - For each image, the required padding is computed. A random split is applied for top/bottom and left/right padding.  \n",
    "   - Corresponding bounding boxes or masks are updated to match the new image dimensions.\n",
    "\n",
    "3. **Final resize to enforce consistent shape (if needed):**  \n",
    "   - A `transforms.Resize((final_max_height, final_max_width))` is applied to each image and target.  \n",
    "   - Often a no-op if the padded size already matches these dimensions, but ensures uniform shape in all images.\n",
    "\n",
    "### Usage Example\n",
    "\n",
    "Because `resize_pad_collate` returns a list of `(image, target)` pairs, you will typically want your dataloader to yield `(images, targets)` as two separate structures. One way to do this is:\n",
    "\n",
    "```python\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Suppose you have a dataset that returns (image, target) tuples\n",
    "my_dataset = ...\n",
    "\n",
    "train_sz = 256  # Example desired max size\n",
    "\n",
    "# We wrap our custom function so that the output splits into two lists:\n",
    "# (images, targets).\n",
    "collate_fn = lambda batch: tuple(zip(*resize_pad_collate(batch, max_sz=train_sz)))\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    my_dataset,\n",
    "    batch_size=4,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "for images, targets in dataloader:\n",
    "    # 'images' is now a tuple of resized & padded images\n",
    "    # 'targets' is a tuple of corresponding dictionaries (or other target structures)\n",
    "    # Each element in 'images' has consistent spatial dimensions\n",
    "    # You can optionally convert them to a list or a torch.stack:\n",
    "    # images = list(images)  # or images = torch.stack(images)\n",
    "    \n",
    "    # Proceed with training loop ...\n",
    "    pass\n",
    "```\n",
    "\n",
    "### Notes & Tips\n",
    "\n",
    "- **Random Padding Benefit:**  \n",
    "  Randomly distributing the padding can help reduce positional bias that might arise if padding were always placed in the same region (e.g., always on the right or bottom).\n",
    "\n",
    "- **Why Round Dimensions to Multiples of 32?**  \n",
    "  Many neural network architectures (especially those using stride-2 convolutions or pooling layers) often produce better or more predictable behavior when processing images of sizes that align with multiples of 32. It can also help with GPU memory management.\n",
    "\n",
    "- **Handling Non-Dict Targets:**  \n",
    "  If your dataset’s target is not a dictionary (or it has a different structure), you’ll need to adapt the function to properly pad and resize those objects.\n",
    "\n",
    "- **Performance Considerations:**  \n",
    "  - These resize and pad operations happen on the CPU. If they become a bottleneck, consider whether you can pre-process the data or move some of these transforms to the GPU.  \n",
    "  - Using random padding every epoch provides a mild data augmentation effect, but also increases CPU workload.\n",
    "\n",
    "This custom collate function is designed to ensure that each sample in a batch has the same size (height and width), which is typically required for training deep learning models, while properly adjusting any bounding boxes or masks in the target data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
