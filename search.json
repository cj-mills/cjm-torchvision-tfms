[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "cjm-torchvision-tfms",
    "section": "",
    "text": "pip install cjm_torchvision_tfms",
    "crumbs": [
      "cjm-torchvision-tfms"
    ]
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "cjm-torchvision-tfms",
    "section": "",
    "text": "pip install cjm_torchvision_tfms",
    "crumbs": [
      "cjm-torchvision-tfms"
    ]
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "cjm-torchvision-tfms",
    "section": "How to use",
    "text": "How to use\n\nfrom PIL import Image\n\nimg_path = './images/call-hand-gesture.png'\n\n# Open the associated image file as a RGB image\nsample_img = Image.open(img_path).convert('RGB')\n\n# Print the dimensions of the image\nprint(f\"Image Dims: {sample_img.size}\")\n\n# Show the image\nsample_img\n\nImage Dims: (384, 512)\n\n\n\n\n\n\n\n\n\n\nfrom cjm_torchvision_tfms.core import ResizeMax, PadSquare, CustomTrivialAugmentWide\n\nimport torch\nfrom torchvision import transforms\nfrom cjm_pytorch_utils.core import tensor_to_pil\nfrom cjm_pil_utils.core import stack_imgs\n\n\ntarget_sz = 384\n\n\nprint(f\"Source image: {sample_img.size}\")\n\n# Create a `ResizeMax` object\nresize_max = ResizeMax(max_sz=target_sz)\n\n# Convert the cropped image to a tensor\nimg_tensor = transforms.PILToTensor()(sample_img)[None]\nprint(f\"Image tensor: {img_tensor.shape}\")\n\n# Resize the tensor\nresized_tensor = resize_max(img_tensor)\nprint(f\"Padded tensor: {resized_tensor.shape}\")\n\n# Display the updated image\ntensor_to_pil(resized_tensor)\n\nSource image: (384, 512)\nImage tensor: torch.Size([1, 3, 512, 384])\nPadded tensor: torch.Size([1, 3, 384, 288])\n\n\n\n\n\n\n\n\n\n\nprint(f\"Resized tensor: {resized_tensor.shape}\")\n\n# Create a `PadSquare` object\npad_square = PadSquare(shift=True)\n\n# Pad the tensor\npadded_tensor = pad_square(resized_tensor)\nprint(f\"Padded tensor: {padded_tensor.shape}\")\n\n# Display the updated image\nstack_imgs([tensor_to_pil(pad_square(resized_tensor)) for i in range(3)])\n\nResized tensor: torch.Size([3, 384, 288])\nPadded tensor: torch.Size([3, 384, 384])\n\n\n\n\n\n\n\n\n\n\nnum_bins = 31\n\ncustom_augmentation_space = {\n    # Identity operation doesn't change the image\n    \"Identity\": (torch.tensor(0.0), False),\n            \n    # Distort the image along the x or y axis, respectively.\n    \"ShearX\": (torch.linspace(0.0, 0.25, num_bins), True),\n    \"ShearY\": (torch.linspace(0.0, 0.25, num_bins), True),\n\n    # Move the image along the x or y axis, respectively.\n    \"TranslateX\": (torch.linspace(0.0, 32.0, num_bins), True),\n    \"TranslateY\": (torch.linspace(0.0, 32.0, num_bins), True),\n\n    # Rotate operation: rotates the image.\n    \"Rotate\": (torch.linspace(0.0, 45.0, num_bins), True),\n\n    # Adjust brightness, color, contrast,and sharpness respectively.\n    \"Brightness\": (torch.linspace(0.0, 0.75, num_bins), True),\n    \"Color\": (torch.linspace(0.0, 0.99, num_bins), True),\n    \"Contrast\": (torch.linspace(0.0, 0.99, num_bins), True),\n    \"Sharpness\": (torch.linspace(0.0, 0.99, num_bins), True),\n\n    # Reduce the number of bits used to express the color in each channel of the image.\n    \"Posterize\": (8 - (torch.arange(num_bins) / ((num_bins - 1) / 6)).round().int(), False),\n\n    # Invert all pixel values above a threshold.\n    \"Solarize\": (torch.linspace(255.0, 0.0, num_bins), False),\n\n    # Maximize the image contrast by setting the darkest color to black and the lightest to white.\n    \"AutoContrast\": (torch.tensor(0.0), False),\n\n    # Equalize the image histogram to improve its contrast.\n    \"Equalize\": (torch.tensor(0.0), False),\n}\n\n# Create a `CustomTrivialAugmentWide` object\ntrivial_aug = CustomTrivialAugmentWide(op_meta=custom_augmentation_space)\n\n# Pad the tensor\naug_tensor = trivial_aug(resized_tensor)\nprint(f\"Augmented tensor: {aug_tensor.shape}\")\n\n# Display the updated image\nstack_imgs([tensor_to_pil(trivial_aug(resized_tensor)) for i in range(3)])\n\nAugmented tensor: torch.Size([3, 384, 288])",
    "crumbs": [
      "cjm-torchvision-tfms"
    ]
  },
  {
    "objectID": "collate_funcs.html",
    "href": "collate_funcs.html",
    "title": "Collate Functions",
    "section": "",
    "text": "source",
    "crumbs": [
      "Collate Functions"
    ]
  },
  {
    "objectID": "collate_funcs.html#function-resize_pad_collatebatch-max_sz256",
    "href": "collate_funcs.html#function-resize_pad_collatebatch-max_sz256",
    "title": "Collate Functions",
    "section": "Function: resize_pad_collate(batch, max_sz=256)",
    "text": "Function: resize_pad_collate(batch, max_sz=256)\nA custom collate function designed for use in a PyTorch DataLoader. It processes a batch of (image, target) pairs by:\n\nResizing each image (and corresponding target data) so that the maximum dimension of the image does not exceed max_sz.\n\nUses the custom ResizeMax transform (from cjm_torchvision_tfms.transforms) which maintains aspect ratio.\n\nDetermining the largest height and width in the batch, rounding them up to the nearest multiple of 32, and randomly padding each image to match those dimensions.\n\nRandom padding is applied on all sides (top, bottom, left, right) of each image, ensuring that every image ends up with the same final size.\nBounding boxes (BoundingBoxes) and masks (Mask) within each target dictionary are adjusted accordingly (shifted and padded).\n\nPerforming a final resize to guarantee that all images have the exact same dimensions.\n\nTypically a no-op if the padding already produces the correct size but ensures consistency in shape.\n\n\n\nArguments\n\nbatch (List[Tuple[Image, dict]]):\nA list of (image, target) pairs.\n\nEach image can be a PIL image, PyTorch tensor image, or a TorchVision tv_tensors.Image.\nThe target is typically a dictionary containing annotation data such as bounding boxes, masks, or other metadata. This function specifically looks for:\n\n\"boxes\" of type tv_tensors.BoundingBoxes\n\"masks\" of type tv_tensors.Mask\nAny other keys in the dictionary are passed through unchanged.\n\n\nmax_sz (int, optional, default=256):\nThe maximum dimension (width or height) to which each image will be resized.\n\nThe aspect ratio of each image is preserved while resizing.\n\n\n\n\nReturns\n\nfinal_pairs (List[Tuple[Image, dict]]):\nA list of (image, target) pairs where:\n\nEach image is at most max_sz in its largest dimension (before final padding).\nEach image is then padded (randomly on all sides) so that all images share the same height and width (rounded up to multiples of 32).\nThe bounding boxes and masks in the target (if any) are updated to reflect the padding.\nA final resize ensures that each image has the same dimensions ((final_max_height, final_max_width)).\n\n\n\n\nHow It Works\n\nResize to max_sz:\nUses ResizeMax, which shrinks the image so its larger side is at most max_sz, preserving the aspect ratio.\nIdentify maximum batch dimensions & pad:\n\nLoops through all resized images to find the largest height and width.\n\nRounds them up to the nearest multiple of 32.\n\nFor each image, the required padding is computed. A random split is applied for top/bottom and left/right padding.\n\nCorresponding bounding boxes or masks are updated to match the new image dimensions.\n\nFinal resize to enforce consistent shape (if needed):\n\nA transforms.Resize((final_max_height, final_max_width)) is applied to each image and target.\n\nOften a no-op if the padded size already matches these dimensions, but ensures uniform shape in all images.\n\n\n\n\nUsage Example\nBecause resize_pad_collate returns a list of (image, target) pairs, you will typically want your dataloader to yield (images, targets) as two separate structures. One way to do this is:\nimport torch\nfrom torch.utils.data import DataLoader\n\n# Suppose you have a dataset that returns (image, target) tuples\nmy_dataset = ...\n\ntrain_sz = 256  # Example desired max size\n\n# We wrap our custom function so that the output splits into two lists:\n# (images, targets).\ncollate_fn = lambda batch: tuple(zip(*resize_pad_collate(batch, max_sz=train_sz)))\n\ndataloader = DataLoader(\n    my_dataset,\n    batch_size=4,\n    collate_fn=collate_fn\n)\n\nfor images, targets in dataloader:\n    # 'images' is now a tuple of resized & padded images\n    # 'targets' is a tuple of corresponding dictionaries (or other target structures)\n    # Each element in 'images' has consistent spatial dimensions\n    # You can optionally convert them to a list or a torch.stack:\n    # images = list(images)  # or images = torch.stack(images)\n    \n    # Proceed with training loop ...\n    pass\n\n\nNotes & Tips\n\nRandom Padding Benefit:\nRandomly distributing the padding can help reduce positional bias that might arise if padding were always placed in the same region (e.g., always on the right or bottom).\nWhy Round Dimensions to Multiples of 32?\nMany neural network architectures (especially those using stride-2 convolutions or pooling layers) often produce better or more predictable behavior when processing images of sizes that align with multiples of 32. It can also help with GPU memory management.\nHandling Non-Dict Targets:\nIf your dataset’s target is not a dictionary (or it has a different structure), you’ll need to adapt the function to properly pad and resize those objects.\nPerformance Considerations:\n\nThese resize and pad operations happen on the CPU. If they become a bottleneck, consider whether you can pre-process the data or move some of these transforms to the GPU.\n\nUsing random padding every epoch provides a mild data augmentation effect, but also increases CPU workload.\n\n\nThis custom collate function is designed to ensure that each sample in a batch has the same size (height and width), which is typically required for training deep learning models, while properly adjusting any bounding boxes or masks in the target data.",
    "crumbs": [
      "Collate Functions"
    ]
  },
  {
    "objectID": "core.html",
    "href": "core.html",
    "title": "core",
    "section": "",
    "text": "img_path = './images/call-hand-gesture.png'\n\n# Open the associated image file as a RGB image\nsample_img = Image.open(img_path).convert('RGB')\n\n# Print the dimensions of the image\nprint(f\"Image Dims: {sample_img.size}\")\n\n# Show the image\nsample_img\n\nImage Dims: (384, 512)\n\n\n\n\n\n\n\n\n\n\nsource\n\nResizeMax\n\n ResizeMax (max_sz:int=256)\n\nA PyTorch Transform class that resizes an image such that the maximum dimension is equal to a specified size while maintaining the aspect ratio.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmax_sz\nint\n256\nThe maximum size for any dimension (height or width) of the image.\n\n\n\n\ntarget_sz = 384\n\n\nprint(f\"Source image: {sample_img.size}\")\n\n# Create a `ResizeMax` object\nresize_max = ResizeMax(max_sz=target_sz)\n\n# Convert the cropped image to a tensor\nimg_tensor = transforms.PILToTensor()(sample_img)[None]\nprint(f\"Image tensor: {img_tensor.shape}\")\n\n# Resize the tensor\nresized_tensor = resize_max(img_tensor)\nprint(f\"Padded tensor: {resized_tensor.shape}\")\n\n# Display the updated image\ntensor_to_pil(resized_tensor)\n\nSource image: (384, 512)\nImage tensor: torch.Size([1, 3, 512, 384])\nPadded tensor: torch.Size([1, 3, 384, 288])\n\n\n\n\n\n\n\n\n\n\nsource\n\n\nPadSquare\n\n PadSquare (padding_mode:str='constant', fill:tuple=(123, 117, 104),\n            shift:bool=True)\n\nPadSquare is a PyTorch Transform class used to pad images to make them square. Depending on the configuration, padding can be applied equally on both sides, or can be randomly split between the two sides.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\npadding_mode\nstr\nconstant\nThe method to use for padding. Default is ‘constant’.\n\n\nfill\ntuple\n(123, 117, 104)\nThe RGB values to use for padding if padding_mode is ‘constant’.\n\n\nshift\nbool\nTrue\nIf True, padding is randomly split between the two sides. If False, padding is equally applied.\n\n\n\n\nprint(f\"Resized tensor: {resized_tensor.shape}\")\n\n# Create a `PadSquare` object\npad_square = PadSquare(shift=True)\n\n# Pad the tensor\npadded_tensor = pad_square(resized_tensor)\nprint(f\"Padded tensor: {padded_tensor.shape}\")\n\n# Display the updated image\nstack_imgs([tensor_to_pil(pad_square(resized_tensor)) for i in range(3)])\n\nResized tensor: torch.Size([3, 384, 288])\nPadded tensor: torch.Size([3, 384, 384])\n\n\n\n\n\n\n\n\n\n\nsource\n\n\nCustomTrivialAugmentWide\n\n CustomTrivialAugmentWide (num_magnitude_bins:int=31, interpolation:torchv\n                           ision.transforms.functional.InterpolationMode=&lt;\n                           InterpolationMode.NEAREST: 'nearest'&gt;,\n                           fill:Optional[List[float]]=None, op_meta:Option\n                           al[Dict[str,Tuple[torch.Tensor,bool]]]=None)\n\nThis class extends the TrivialAugmentWide class provided by PyTorch’s transforms module. TrivialAugmentWide is an augmentation policy randomly applies a single augmentation to each image.\n\nnum_bins = 31\n\ncustom_augmentation_space = {\n    # Identity operation doesn't change the image\n    \"Identity\": (torch.tensor(0.0), False),\n            \n    # Distort the image along the x or y axis, respectively.\n    \"ShearX\": (torch.linspace(0.0, 0.25, num_bins), True),\n    \"ShearY\": (torch.linspace(0.0, 0.25, num_bins), True),\n\n    # Move the image along the x or y axis, respectively.\n    \"TranslateX\": (torch.linspace(0.0, 32.0, num_bins), True),\n    \"TranslateY\": (torch.linspace(0.0, 32.0, num_bins), True),\n\n    # Rotate operation: rotates the image.\n    \"Rotate\": (torch.linspace(0.0, 45.0, num_bins), True),\n\n    # Adjust brightness, color, contrast,and sharpness respectively.\n    \"Brightness\": (torch.linspace(0.0, 0.75, num_bins), True),\n    \"Color\": (torch.linspace(0.0, 0.99, num_bins), True),\n    \"Contrast\": (torch.linspace(0.0, 0.99, num_bins), True),\n    \"Sharpness\": (torch.linspace(0.0, 0.99, num_bins), True),\n\n    # Reduce the number of bits used to express the color in each channel of the image.\n    \"Posterize\": (8 - (torch.arange(num_bins) / ((num_bins - 1) / 6)).round().int(), False),\n\n    # Invert all pixel values above a threshold.\n    \"Solarize\": (torch.linspace(255.0, 0.0, num_bins), False),\n\n    # Maximize the image contrast by setting the darkest color to black and the lightest to white.\n    \"AutoContrast\": (torch.tensor(0.0), False),\n\n    # Equalize the image histogram to improve its contrast.\n    \"Equalize\": (torch.tensor(0.0), False),\n}\n\n\n# Create a `CustomTrivialAugmentWide` object\ntrivial_aug = CustomTrivialAugmentWide(op_meta=custom_augmentation_space)\n\n# Pad the tensor\naug_tensor = trivial_aug(resized_tensor)\nprint(f\"Augmented tensor: {aug_tensor.shape}\")\n\n# Display the updated image\nstack_imgs([tensor_to_pil(trivial_aug(resized_tensor)) for i in range(3)])\n\nAugmented tensor: torch.Size([3, 384, 288])\n\n\n\n\n\n\n\n\n\n\nsource\n\n\nCustomRandomIoUCrop\n\n CustomRandomIoUCrop (min_scale:float=0.3, max_scale:float=1.0,\n                      min_aspect_ratio:float=0.5,\n                      max_aspect_ratio:float=2.0,\n                      sampler_options:Optional[List[float]]=[0.0, 0.1,\n                      0.3, 0.5, 0.7, 0.9, 1.0], trials:int=40,\n                      jitter_factor:float=0.0)\n\nA customized Random IoU crop transformation that inherits from torchvision’s RandomIoUCrop transform.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmin_scale\nfloat\n0.3\nMinimum factors to scale the input size.\n\n\nmax_scale\nfloat\n1.0\nMaximum factors to scale the input size.\n\n\nmin_aspect_ratio\nfloat\n0.5\nMinimum aspect ratio for the cropped image or video.\n\n\nmax_aspect_ratio\nfloat\n2.0\nMaximum aspect ratio for the cropped image or video.\n\n\nsampler_options\nOptional\n[0.0, 0.1, 0.3, 0.5, 0.7, 0.9, 1.0]\nList of minimal IoU (Jaccard) overlap between all the boxes a cropped image or video.\n\n\ntrials\nint\n40\nNumber of trials to find a crop for a given value of minimal IoU (Jaccard) overlap.\n\n\njitter_factor\nfloat\n0.0\nValue to jitter the center coordinates for the crop area.\n\n\n\n\nclass_names = ['call']\n\n\n# Prepare bounding box targets\ntargets = {\n    'boxes': BoundingBoxes([[ 91.8727, 146.4079, 188.0844, 252.7894]], \n                         format=BoundingBoxFormat.XYXY, \n                         canvas_size=sample_img.size[::-1]),\n    'labels': torch.Tensor([class_names.index(label) for label in ['call']])\n}\n# Annotate the sample image with labels and bounding boxes\nannotated_tensor = draw_bounding_boxes(\n    image=transforms.PILToTensor()(sample_img), \n    boxes=targets['boxes'], \n    labels=[class_names[int(label.item())] for label in targets['labels']],\n)\n\ndisplay(tensor_to_pil(annotated_tensor))\n\n\n\n\n\n\n\n\n\n# Create a RandomIoUCrop object\niou_crop = CustomRandomIoUCrop(min_scale=0.3, \n                                  max_scale=1.0, \n                                  min_aspect_ratio=0.5, \n                                  max_aspect_ratio=2.0, \n                                  sampler_options=[0.0, 0.1, 0.3, 0.5, 0.7, 0.9, 1.0],\n                                  trials=400, \n                                  jitter_factor=0.25)\n\n# Crop the image\ncropped_img, cropped_targets = iou_crop(sample_img, targets)\nsanitized_img, sanitized_targets = transforms.SanitizeBoundingBoxes()(cropped_img, cropped_targets)\n\n# Annotate the augmented image with updated labels and bounding boxes\nannotated_tensor = draw_bounding_boxes(\n    image=transforms.PILToTensor()(sanitized_img), \n    boxes=cropped_targets['boxes'], \n    labels=[class_names[int(label.item())] for label in sanitized_targets['labels']],\n)\n\ntensor_to_pil(annotated_tensor)\n\n\n\n\n\n\n\n\n\nsource\n\n\nRandomPatchCopy\n\n RandomPatchCopy (pct:float=0.2, min_num:int=0, max_num:int=4,\n                  iou_thresh:float=0.25)\n\nA torchvision V2 transform that copies data from a randomly selected rectangular patch to another randomly selected rectangular region of an image tensor multiple times.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\npct\nfloat\n0.2\nThe percentage of the tensor’s size to be used as the side length of the square regions.\n\n\nmin_num\nint\n0\nThe minimum number of times to apply the rand_square_copy function.\n\n\nmax_num\nint\n4\nThe maximum number of times to apply the rand_square_copy function.\n\n\niou_thresh\nfloat\n0.25\nThe IoU threshold for bounding box suppression.\n\n\n\n\n# Create a RandomPatchCopy object\nrand_patch_copy_tfm = RandomPatchCopy(pct=0.3, min_num=1, max_num=4)\n\n# Feed sample image and targets through the image transform\naugmented_img, augmented_targets = rand_patch_copy_tfm(sample_img, targets)\n# Remove degenerate/invalid bounding boxes and their corresponding labels and masks.\nsanitized_img, sanitized_targets = transforms.SanitizeBoundingBoxes()(augmented_img, augmented_targets)\n\n# Annotate the augmented image with updated labels and bounding boxes\nannotated_tensor = draw_bounding_boxes(\n    image=transforms.PILToTensor()(sanitized_img), \n    boxes=sanitized_targets['boxes'], \n    labels=[class_names[int(label.item())] for label in sanitized_targets['labels']], \n)\n\n# Display the augmented image\ntensor_to_pil(annotated_tensor)\n\n\n\n\n\n\n\n\n\nsource\n\n\nRandomPixelCopy\n\n RandomPixelCopy (min_pct=0.0025, max_pct:float=0.1)\n\nA torchvision V2 transform that copies data from a randomly selected set of pixels to another randomly selected set of pixels of a image tensor.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmin_pct\nfloat\n0.0025\nThe minimum percentage of the tensor’s pixels to be copied.\n\n\nmax_pct\nfloat\n0.1\nThe maximum percentage of the tensor’s pixels to be copied.\n\n\n\n\n# Create a RandomPixelCopy object\nrand_pixel_copy_tfm = RandomPixelCopy(max_pct=0.05)\n\n# Feed sample image and targets through the image transform\naugmented_img, augmented_targets = rand_pixel_copy_tfm(sample_img, targets)\n\n# Annotate the augmented image with updated labels and bounding boxes\nannotated_tensor = draw_bounding_boxes(\n    image=transforms.PILToTensor()(augmented_img), \n    boxes=augmented_targets['boxes'], \n    labels=[class_names[int(label.item())] for label in augmented_targets['labels']], \n)\n\n# Display the augmented image\ntransforms.ToPILImage()(annotated_tensor)\n\n\n\n\n\n\n\n\n\nsource\n\n\nCustomRandomAugment\n\n CustomRandomAugment (num_bins:int=31, shear:int=18, translate:float=0.25,\n                      degrees:float=70.0, brightness:float=0.75,\n                      hue:float=0.4, saturation:float=0.99,\n                      contrast:float=0.75, sharpness:tuple=(0.0, 1.99),\n                      posterize:tuple=(2.0, 8.0), solarize:tuple=(1.0,\n                      255.0), auto_contrast:bool=True, equalize:bool=True)\n\nA PyTorch Module for applying a random augmentation from a predefined set of augmentations to an image and its associated targets (e.g., bounding boxes, masks). This class is designed to work with torchvision transforms and supports augmentation of both images and their corresponding target data.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nnum_bins\nint\n31\nThe number of discrete levels for certain augmentations.\n\n\nshear\nint\n18\nMaximum shear angle for the RandomAffine transform.\n\n\ntranslate\nfloat\n0.25\nMaximum translation as a fraction of image dimensions for RandomAffine.\n\n\ndegrees\nfloat\n70.0\nRange of degrees for random rotations.\n\n\nbrightness\nfloat\n0.75\nBrightness adjustment factor.\n\n\nhue\nfloat\n0.4\nHue adjustment range.\n\n\nsaturation\nfloat\n0.99\nSaturation adjustment factor.\n\n\ncontrast\nfloat\n0.75\nContrast adjustment factor.\n\n\nsharpness\ntuple\n(0.0, 1.99)\nRange for sharpness factor adjustments.\n\n\nposterize\ntuple\n(2.0, 8.0)\nRange for bits in posterization.\n\n\nsolarize\ntuple\n(1.0, 255.0)\nThreshold range for solarization.\n\n\nauto_contrast\nbool\nTrue\n\n\n\nequalize\nbool\nTrue\n\n\n\n\n\n# Create a RandomIoUCrop object\nrandom_aug_tfm = CustomRandomAugment()\n\n# Augment the image\naugmented_img, augmented_targets = random_aug_tfm(sample_img, targets)\nsanitized_img, sanitized_targets = transforms.SanitizeBoundingBoxes()(augmented_img, augmented_targets)\n\n# Annotate the augmented image with updated labels and bounding boxes\nannotated_tensor = draw_bounding_boxes(\n    image=transforms.PILToTensor()(sanitized_img), \n    boxes=sanitized_targets['boxes'], \n    labels=[class_names[int(label.item())] for label in sanitized_targets['labels']],\n)\n\ntensor_to_pil(annotated_tensor)\n\n\n\n\n\n\n\n\n\nsource\n\n\nAddLightGlare\n\n AddLightGlare (threshold_range=(180, 220), blur_radius_range=(5, 20),\n                intensity_range=(0.5, 1.0))\n\nA custom torchvision V2 transform that adds a simple bloom/glare effect to images. All parameters can optionally be randomized from user-specified ranges.",
    "crumbs": [
      "core"
    ]
  },
  {
    "objectID": "transforms.html",
    "href": "transforms.html",
    "title": "transforms",
    "section": "",
    "text": "img_path = './images/call-hand-gesture.png'\n\n# Open the associated image file as a RGB image\nsample_img = Image.open(img_path).convert('RGB')\n\n# Print the dimensions of the image\nprint(f\"Image Dims: {sample_img.size}\")\n\n# Show the image\nsample_img\n\nImage Dims: (384, 512)\n\n\n\n\n\n\n\n\n\n\nsource\n\nResizeMax\n\n ResizeMax (max_sz:int=256)\n\nA PyTorch Transform class that resizes an image such that the maximum dimension is equal to a specified size while maintaining the aspect ratio.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmax_sz\nint\n256\nThe maximum size for any dimension (height or width) of the image.\n\n\n\n\ntarget_sz = 384\n\n\nprint(f\"Source image: {sample_img.size}\")\n\n# Create a `ResizeMax` object\nresize_max = ResizeMax(max_sz=target_sz)\n\n# Convert the cropped image to a tensor\nimg_tensor = transforms.PILToTensor()(sample_img)[None]\nprint(f\"Image tensor: {img_tensor.shape}\")\n\n# Resize the tensor\nresized_tensor = resize_max(img_tensor)\nprint(f\"Padded tensor: {resized_tensor.shape}\")\n\n# Display the updated image\ntensor_to_pil(resized_tensor)\n\nSource image: (384, 512)\nImage tensor: torch.Size([1, 3, 512, 384])\nPadded tensor: torch.Size([1, 3, 384, 288])\n\n\n\n\n\n\n\n\n\n\nsource\n\n\nPadSquare\n\n PadSquare (padding_mode:str='constant', fill:tuple=(123, 117, 104),\n            shift:bool=True)\n\nPadSquare is a PyTorch Transform class used to pad images to make them square. Depending on the configuration, padding can be applied equally on both sides, or can be randomly split between the two sides.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\npadding_mode\nstr\nconstant\nThe method to use for padding. Default is ‘constant’.\n\n\nfill\ntuple\n(123, 117, 104)\nThe RGB values to use for padding if padding_mode is ‘constant’.\n\n\nshift\nbool\nTrue\nIf True, padding is randomly split between the two sides. If False, padding is equally applied.\n\n\n\n\nprint(f\"Resized tensor: {resized_tensor.shape}\")\n\n# Create a `PadSquare` object\npad_square = PadSquare(shift=True)\n\n# Pad the tensor\npadded_tensor = pad_square(resized_tensor)\nprint(f\"Padded tensor: {padded_tensor.shape}\")\n\n# Display the updated image\nstack_imgs([tensor_to_pil(pad_square(resized_tensor)) for i in range(3)])\n\nResized tensor: torch.Size([3, 384, 288])\nPadded tensor: torch.Size([3, 384, 384])\n\n\n\n\n\n\n\n\n\n\nsource\n\n\nCustomTrivialAugmentWide\n\n CustomTrivialAugmentWide (num_magnitude_bins:int=31, interpolation:torchv\n                           ision.transforms.functional.InterpolationMode=&lt;\n                           InterpolationMode.NEAREST: 'nearest'&gt;,\n                           fill:Optional[List[float]]=None, op_meta:Option\n                           al[Dict[str,Tuple[torch.Tensor,bool]]]=None)\n\nThis class extends the TrivialAugmentWide class provided by PyTorch’s transforms module. TrivialAugmentWide is an augmentation policy randomly applies a single augmentation to each image.\n\nnum_bins = 31\n\ncustom_augmentation_space = {\n    # Identity operation doesn't change the image\n    \"Identity\": (torch.tensor(0.0), False),\n            \n    # Distort the image along the x or y axis, respectively.\n    \"ShearX\": (torch.linspace(0.0, 0.25, num_bins), True),\n    \"ShearY\": (torch.linspace(0.0, 0.25, num_bins), True),\n\n    # Move the image along the x or y axis, respectively.\n    \"TranslateX\": (torch.linspace(0.0, 32.0, num_bins), True),\n    \"TranslateY\": (torch.linspace(0.0, 32.0, num_bins), True),\n\n    # Rotate operation: rotates the image.\n    \"Rotate\": (torch.linspace(0.0, 45.0, num_bins), True),\n\n    # Adjust brightness, color, contrast,and sharpness respectively.\n    \"Brightness\": (torch.linspace(0.0, 0.75, num_bins), True),\n    \"Color\": (torch.linspace(0.0, 0.99, num_bins), True),\n    \"Contrast\": (torch.linspace(0.0, 0.99, num_bins), True),\n    \"Sharpness\": (torch.linspace(0.0, 0.99, num_bins), True),\n\n    # Reduce the number of bits used to express the color in each channel of the image.\n    \"Posterize\": (8 - (torch.arange(num_bins) / ((num_bins - 1) / 6)).round().int(), False),\n\n    # Invert all pixel values above a threshold.\n    \"Solarize\": (torch.linspace(255.0, 0.0, num_bins), False),\n\n    # Maximize the image contrast by setting the darkest color to black and the lightest to white.\n    \"AutoContrast\": (torch.tensor(0.0), False),\n\n    # Equalize the image histogram to improve its contrast.\n    \"Equalize\": (torch.tensor(0.0), False),\n}\n\n\n# Create a `CustomTrivialAugmentWide` object\ntrivial_aug = CustomTrivialAugmentWide(op_meta=custom_augmentation_space)\n\n# Pad the tensor\naug_tensor = trivial_aug(resized_tensor)\nprint(f\"Augmented tensor: {aug_tensor.shape}\")\n\n# Display the updated image\nstack_imgs([tensor_to_pil(trivial_aug(resized_tensor)) for i in range(3)])\n\nAugmented tensor: torch.Size([3, 384, 288])\n\n\n\n\n\n\n\n\n\n\nsource\n\n\nCustomRandomIoUCrop\n\n CustomRandomIoUCrop (min_scale:float=0.3, max_scale:float=1.0,\n                      min_aspect_ratio:float=0.5,\n                      max_aspect_ratio:float=2.0,\n                      sampler_options:Optional[List[float]]=[0.0, 0.1,\n                      0.3, 0.5, 0.7, 0.9, 1.0], trials:int=40,\n                      jitter_factor:float=0.0)\n\nA customized Random IoU crop transformation that inherits from torchvision’s RandomIoUCrop transform.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmin_scale\nfloat\n0.3\nMinimum factors to scale the input size.\n\n\nmax_scale\nfloat\n1.0\nMaximum factors to scale the input size.\n\n\nmin_aspect_ratio\nfloat\n0.5\nMinimum aspect ratio for the cropped image or video.\n\n\nmax_aspect_ratio\nfloat\n2.0\nMaximum aspect ratio for the cropped image or video.\n\n\nsampler_options\nOptional\n[0.0, 0.1, 0.3, 0.5, 0.7, 0.9, 1.0]\nList of minimal IoU (Jaccard) overlap between all the boxes a cropped image or video.\n\n\ntrials\nint\n40\nNumber of trials to find a crop for a given value of minimal IoU (Jaccard) overlap.\n\n\njitter_factor\nfloat\n0.0\nValue to jitter the center coordinates for the crop area.\n\n\n\n\nclass_names = ['call']\n\n\n# Prepare bounding box targets\ntargets = {\n    'boxes': BoundingBoxes([[ 91.8727, 146.4079, 188.0844, 252.7894]], \n                         format=BoundingBoxFormat.XYXY, \n                         canvas_size=sample_img.size[::-1]),\n    'labels': torch.Tensor([class_names.index(label) for label in ['call']])\n}\n# Annotate the sample image with labels and bounding boxes\nannotated_tensor = draw_bounding_boxes(\n    image=transforms.PILToTensor()(sample_img), \n    boxes=targets['boxes'], \n    labels=[class_names[int(label.item())] for label in targets['labels']],\n)\n\ndisplay(tensor_to_pil(annotated_tensor))\n\n\n\n\n\n\n\n\n\n# Create a RandomIoUCrop object\niou_crop = CustomRandomIoUCrop(min_scale=0.3, \n                                  max_scale=1.0, \n                                  min_aspect_ratio=0.5, \n                                  max_aspect_ratio=2.0, \n                                  sampler_options=[0.0, 0.1, 0.3, 0.5, 0.7, 0.9, 1.0],\n                                  trials=400, \n                                  jitter_factor=0.25)\n\n# Crop the image\ncropped_img, cropped_targets = iou_crop(sample_img, targets)\nsanitized_img, sanitized_targets = transforms.SanitizeBoundingBoxes()(cropped_img, cropped_targets)\n\n# Annotate the augmented image with updated labels and bounding boxes\nannotated_tensor = draw_bounding_boxes(\n    image=transforms.PILToTensor()(sanitized_img), \n    boxes=cropped_targets['boxes'], \n    labels=[class_names[int(label.item())] for label in sanitized_targets['labels']],\n)\n\ntensor_to_pil(annotated_tensor)\n\n\n\n\n\n\n\n\n\nsource\n\n\nRandomPatchCopy\n\n RandomPatchCopy (pct:float=0.2, min_num:int=0, max_num:int=4,\n                  iou_thresh:float=0.25)\n\nA torchvision V2 transform that copies data from a randomly selected rectangular patch to another randomly selected rectangular region of an image tensor multiple times.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\npct\nfloat\n0.2\nThe percentage of the tensor’s size to be used as the side length of the square regions.\n\n\nmin_num\nint\n0\nThe minimum number of times to apply the rand_square_copy function.\n\n\nmax_num\nint\n4\nThe maximum number of times to apply the rand_square_copy function.\n\n\niou_thresh\nfloat\n0.25\nThe IoU threshold for bounding box suppression.\n\n\n\n\n# Create a RandomPatchCopy object\nrand_patch_copy_tfm = RandomPatchCopy(pct=0.3, min_num=1, max_num=4)\n\n# Feed sample image and targets through the image transform\naugmented_img, augmented_targets = rand_patch_copy_tfm(sample_img, targets)\n# Remove degenerate/invalid bounding boxes and their corresponding labels and masks.\nsanitized_img, sanitized_targets = transforms.SanitizeBoundingBoxes()(augmented_img, augmented_targets)\n\n# Annotate the augmented image with updated labels and bounding boxes\nannotated_tensor = draw_bounding_boxes(\n    image=transforms.PILToTensor()(sanitized_img), \n    boxes=sanitized_targets['boxes'], \n    labels=[class_names[int(label.item())] for label in sanitized_targets['labels']], \n)\n\n# Display the augmented image\ntensor_to_pil(annotated_tensor)\n\n/home/innom-dt/miniforge3/envs/cjm-torchvision-tfms-testing/lib/python3.11/site-packages/torchvision/utils.py:215: UserWarning: boxes doesn't contain any box. No box was drawn\n  warnings.warn(\"boxes doesn't contain any box. No box was drawn\")\n\n\n\n\n\n\n\n\n\n\nsource\n\n\nRandomPixelCopy\n\n RandomPixelCopy (min_pct=0.0025, max_pct:float=0.1)\n\nA torchvision V2 transform that copies data from a randomly selected set of pixels to another randomly selected set of pixels of a image tensor.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmin_pct\nfloat\n0.0025\nThe minimum percentage of the tensor’s pixels to be copied.\n\n\nmax_pct\nfloat\n0.1\nThe maximum percentage of the tensor’s pixels to be copied.\n\n\n\n\n# Create a RandomPixelCopy object\nrand_pixel_copy_tfm = RandomPixelCopy(max_pct=0.05)\n\n# Feed sample image and targets through the image transform\naugmented_img, augmented_targets = rand_pixel_copy_tfm(sample_img, targets)\n\n# Annotate the augmented image with updated labels and bounding boxes\nannotated_tensor = draw_bounding_boxes(\n    image=transforms.PILToTensor()(augmented_img), \n    boxes=augmented_targets['boxes'], \n    labels=[class_names[int(label.item())] for label in augmented_targets['labels']], \n)\n\n# Display the augmented image\ntransforms.ToPILImage()(annotated_tensor)\n\n\n\n\n\n\n\n\n\nsource\n\n\nCustomRandomAugment\n\n CustomRandomAugment (num_bins:int=31, shear:int=18, translate:float=0.25,\n                      degrees:float=70.0, brightness:float=0.75,\n                      hue:float=0.4, saturation:float=0.99,\n                      contrast:float=0.75, sharpness:tuple=(0.0, 1.99),\n                      posterize:tuple=(2.0, 8.0), solarize:tuple=(1.0,\n                      255.0), auto_contrast:bool=True, equalize:bool=True)\n\nA PyTorch Module for applying a random augmentation from a predefined set of augmentations to an image and its associated targets (e.g., bounding boxes, masks). This class is designed to work with torchvision transforms and supports augmentation of both images and their corresponding target data.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nnum_bins\nint\n31\nThe number of discrete levels for certain augmentations.\n\n\nshear\nint\n18\nMaximum shear angle for the RandomAffine transform.\n\n\ntranslate\nfloat\n0.25\nMaximum translation as a fraction of image dimensions for RandomAffine.\n\n\ndegrees\nfloat\n70.0\nRange of degrees for random rotations.\n\n\nbrightness\nfloat\n0.75\nBrightness adjustment factor.\n\n\nhue\nfloat\n0.4\nHue adjustment range.\n\n\nsaturation\nfloat\n0.99\nSaturation adjustment factor.\n\n\ncontrast\nfloat\n0.75\nContrast adjustment factor.\n\n\nsharpness\ntuple\n(0.0, 1.99)\nRange for sharpness factor adjustments.\n\n\nposterize\ntuple\n(2.0, 8.0)\nRange for bits in posterization.\n\n\nsolarize\ntuple\n(1.0, 255.0)\nThreshold range for solarization.\n\n\nauto_contrast\nbool\nTrue\n\n\n\nequalize\nbool\nTrue\n\n\n\n\n\n# Create a RandomIoUCrop object\nrandom_aug_tfm = CustomRandomAugment()\n\n# Augment the image\naugmented_img, augmented_targets = random_aug_tfm(sample_img, targets)\nsanitized_img, sanitized_targets = transforms.SanitizeBoundingBoxes()(augmented_img, augmented_targets)\n\n# Annotate the augmented image with updated labels and bounding boxes\nannotated_tensor = draw_bounding_boxes(\n    image=transforms.PILToTensor()(sanitized_img), \n    boxes=sanitized_targets['boxes'], \n    labels=[class_names[int(label.item())] for label in sanitized_targets['labels']],\n)\n\ntensor_to_pil(annotated_tensor)\n\n\n\n\n\n\n\n\n\nsource\n\n\nAddLightGlare\n\n AddLightGlare (threshold_range=(180, 220), blur_radius_range=(5, 20),\n                intensity_range=(0.5, 1.0))\n\nAdds a simple bloom/glare effect to images. All parameters can optionally be randomized from user-specified ranges.\n\nsource\n\n\nAddHaze\n\n AddHaze (blur_radius_range=(2, 6), fog_intensity_range=(0.2, 0.5),\n          contrast_factor_range=(0.7, 0.9))\n\nApplies a haze/fog effect to images. Each parameter can be sampled from user-provided ranges. For fog_color, we pick a single channel (0..255) and use it for (R, G, B).\n\nsource\n\n\nRandomPerspectiveOpenCV\n\n RandomPerspectiveOpenCV (distortion_scale=0.5, p=0.5, fill=0,\n                          min_area=1.0, max_attempts=10)\n\nRandomly applies a perspective transformation to an image (and its associated target data) using OpenCV.\n\nsource\n\n\nRandomPerspectiveCrop\n\n RandomPerspectiveCrop (distortion_scale=0.5, p=0.5, fill=0,\n                        crop_strategy='all_nonzero')\n\nRandomly applies a perspective transform to an image (and its target data), then crops the result based on a chosen strategy. This transform is useful for simulating complex distortions while ensuring the final image is neatly cropped to valid (non-distorted) areas.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndistortion_scale\nfloat\n0.5\n\n\n\np\nfloat\n0.5\n\n\n\nfill\nint\n0\n\n\n\ncrop_strategy\nstr\nall_nonzero\nor “largest_rectangle”\n\n\n\n\nsource\n\n\nRandomRotationCrop\n\n RandomRotationCrop (degrees, p=0.5, fill=0, expand=False, center=None,\n                     crop_strategy='all_nonzero')\n\nRandomly applies a rotation to an image (and its target data), then crops the result based on a chosen strategy. Similar to RandomPerspectiveCrop, but wraps RandomRotation instead.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndegrees\n\n\n\n\n\np\nfloat\n0.5\n\n\n\nfill\nint\n0\n\n\n\nexpand\nbool\nFalse\n\n\n\ncenter\nNoneType\nNone\n\n\n\ncrop_strategy\nstr\nall_nonzero\nor “largest_rectangle”\n\n\n\n\nsource\n\n\nRandomOneDimResize\n\n RandomOneDimResize (scale_range=(0.5, 1.5),\n                     interpolation=&lt;InterpolationMode.BILINEAR:\n                     'bilinear'&gt;, antialias=True)\n\nA PyTorch Transform that randomly scales only one dimension (height or width) of an image by a random factor within a specified range, while keeping the other dimension unchanged.",
    "crumbs": [
      "transforms"
    ]
  },
  {
    "objectID": "utils.html",
    "href": "utils.html",
    "title": "Utilities",
    "section": "",
    "text": "source\n\nround_up_to_multiple\n\n round_up_to_multiple (val, divisor=32)\n\nRound val up to the nearest multiple of divisor.\n\nsource\n\n\nfind_largest_rectangle_coords\n\n find_largest_rectangle_coords (binary_mask:numpy.ndarray)\n\n*Use a row-by-row ‘largest rectangle in histogram’ approach to find the largest rectangle of True cells in a 2D binary numpy array.\nReturns: (row_slice, col_slice) indicating the largest rectangle found.*\n\nsource\n\n\nlargest_rectangle_in_histogram\n\n largest_rectangle_in_histogram (row_heights:numpy.ndarray)\n\n*Given a 1D array ‘row_heights’, returns the area of the largest rectangle in this histogram plus the (top, bottom, left, right) indices.\nNote: In this context, ‘top’ and ‘bottom’ are placeholders, because the actual vertical extent is determined later using the 2D ‘heights’ array.*",
    "crumbs": [
      "Utilities"
    ]
  }
]